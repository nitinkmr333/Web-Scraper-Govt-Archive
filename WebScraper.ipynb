{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup #for data scraping\n",
    "import requests #for bs4\n",
    "import re #for regular expressions\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading data for nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nitin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nitin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') #downloading nltk stopwords\n",
    "nltk.download('punkt') #for sent_tokenize and word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read excel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                   SECFNAME  \n",
       "0  edgar/data/3662/0000950170-98-000413.txt  \n",
       "1  edgar/data/3662/0000950170-98-001001.txt  \n",
       "2  edgar/data/3662/0000950172-98-000783.txt  \n",
       "3  edgar/data/3662/0000950170-98-002145.txt  \n",
       "4  edgar/data/3662/0000950172-98-001203.txt  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('cik_list.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 152 entries, 0 to 151\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   CIK       152 non-null    int64         \n",
      " 1   CONAME    152 non-null    object        \n",
      " 2   FYRMO     152 non-null    int64         \n",
      " 3   FDATE     152 non-null    datetime64[ns]\n",
      " 4   FORM      152 non-null    object        \n",
      " 5   SECFNAME  152 non-null    object        \n",
      "dtypes: datetime64[ns](1), int64(2), object(3)\n",
      "memory usage: 7.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIK         0\n",
       "CONAME      0\n",
       "FYRMO       0\n",
       "FDATE       0\n",
       "FORM        0\n",
       "SECFNAME    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No null values found.\n",
    "\n",
    "Next, completing urls by adding \"https://www.sec.gov/Archives/\" to in front each value of \"SECFNAME\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_url = \"https://www.sec.gov/Archives/\"\n",
    "df_orig = df.copy(deep=True) #creating copy of original dataframe for later use\n",
    "df['SECFNAME'] = starting_url + df['SECFNAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating data lists (using dataframes) from excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_constraining = pd.read_excel('constraining_dictionary.xlsx') #header = word\n",
    "dict_constraining = dict_constraining['Word'].tolist() #converting dataframe to list\n",
    "dict_constraining = [word.lower() for word in dict_constraining] #converting to lowercase\n",
    "\n",
    "dict_uncertainty = pd.read_excel('uncertainty_dictionary.xlsx') #header = word\n",
    "dict_uncertainty = dict_uncertainty['Word'].tolist() #converting dataframe to list\n",
    "dict_uncertainty = [word.lower() for word in dict_uncertainty] #converting to lowercase\n",
    "\n",
    "dict_stopwords_generic = pd.read_csv('extra/StopWords_Generic.txt', header=None) #no headers in this text file\n",
    "dict_stopwords_generic = dict_stopwords_generic.iloc[:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next is master dictionary which contains positive and negative words\n",
    "filepath = 'extra/LoughranMcDonald_SentimentWordLists_2018.xlsx'\n",
    "\n",
    "#for postive word list\n",
    "dict_sentiment_word_lists_positive = pd.read_excel(filepath, sheet_name = 'Positive', header=None)\n",
    "dict_sentiment_word_lists_positive = dict_sentiment_word_lists_positive[0].tolist() #converting dataframe to list\n",
    "dict_sentiment_word_lists_positive = [word.lower() for word in dict_sentiment_word_lists_positive] #converting to lowercase\n",
    "\n",
    "#for negative word list\n",
    "dict_sentiment_word_lists_negative = pd.read_excel(filepath, sheet_name = 'Negative', header=None)\n",
    "dict_sentiment_word_lists_negative = dict_sentiment_word_lists_negative[0].tolist() #converting dataframe to list\n",
    "dict_sentiment_word_lists_negative = [word.lower() for word in dict_sentiment_word_lists_negative] #converting to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to be used for extraction, cleaning and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_bs4(url):\n",
    "    html_content = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_content, \"lxml\") #requires lxml package\n",
    "    return soup.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean html data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    cleaned_data = re.sub('[^A-Za-z \\.\\'\\?]+','',data) #removing digits, special characters except \"? '.\"\n",
    "    cleaned_data = re.sub(' +',' ',cleaned_data) #replacing one or more whitespaces with a single space\n",
    "    cleaned_data = re.sub(' \\.','.',cleaned_data) #replacing \" .\" with \".\"\n",
    "    cleaned_data = re.sub('\\.+','.',cleaned_data) #replacing multiple \".\" with a single \".\"\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(stop_words,cleaned_word_tokens):\n",
    "    removed_list_stopwords = [word for word in cleaned_word_tokens if not word in stop_words] \n",
    "    return removed_list_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count syllables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllable(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if word is complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_complex(word):\n",
    "    count = count_syllable(word)\n",
    "    if count > 2:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to be calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_positive_score(cleaned_data_words):\n",
    "    count = 0\n",
    "    for word in cleaned_data_words:\n",
    "        if word in dict_sentiment_word_lists_positive:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_negative_score(cleaned_data_words):\n",
    "    count = 0\n",
    "    for word in cleaned_data_words:\n",
    "        if word in dict_sentiment_word_lists_negative:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polarity score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_polarity_score(positive_score, negative_score):\n",
    "    return (positive_score - negative_score)/((positive_score + negative_score) + 0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average sentence length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_avg_len(no_of_words, no_of_sentences):\n",
    "    return no_of_words/no_of_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of complex words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_complex_words(no_complex_words, no_of_words):\n",
    "    return no_complex_words/no_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fog index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fog_index(avg_sentence_length, percent_complex):\n",
    "    return 0.4*(avg_sentence_length + percent_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_uncertainty_score(cleaned_data_words):\n",
    "    count = 0\n",
    "    for word in cleaned_data_words:\n",
    "        if word in dict_uncertainty:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constraining score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_constraining_score(cleaned_data_words):\n",
    "    count = 0\n",
    "    for word in cleaned_data_words:\n",
    "        if word in dict_constraining:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constraining words for the whole report (only calculated once) for each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraining_words_whole_report(data):\n",
    "    cleaned_data = clean_data(data)\n",
    "    cleaned_data = re.sub('[\\.\\?]','',cleaned_data) #removing '.' and '?' for words\n",
    "    word_tokens = word_tokenize(cleaned_data) #tokenize words \n",
    "    cleaned_data_words = [word for word in word_tokens if word] #remove empty string values\n",
    "    cleaned_data_words = [word.lower() for word in cleaned_data_words] #converting all words into lowercase\n",
    "    \n",
    "    count = 0\n",
    "    for word in cleaned_data_words:\n",
    "        if word in dict_constraining:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN PART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14 variables to be calculated for 3 sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_names = ['mda','qqdmr','rf']\n",
    "header_columns = ['positive_score','negative_score','polarity_score','average_sentence_length', 'percentage_of_complex_words','fog_index','complex_word_count','word_count','uncertainty_score','constraining_score','positive_word_proportion','negative_word_proportion','uncertainty_word_proportion','constraining_word_proportion']\n",
    "section_full_names = ['MANAGEMENT\\'S DISCUSSION AND ANALYSIS','QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK','RISK FACTORS']\n",
    "\n",
    "header_columns_with_sections = []\n",
    "# all combinations\n",
    "for name in section_names:\n",
    "    for column in header_columns:\n",
    "        header_columns_with_sections.append(name + '_' + column)\n",
    "\n",
    "# finally appending last column\n",
    "header_columns_with_sections.append('constraining_words_whole_report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new dataframe to store final values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "      <th>mda_positive_score</th>\n",
       "      <th>mda_negative_score</th>\n",
       "      <th>mda_polarity_score</th>\n",
       "      <th>mda_average_sentence_length</th>\n",
       "      <th>...</th>\n",
       "      <th>rf_fog_index</th>\n",
       "      <th>rf_complex_word_count</th>\n",
       "      <th>rf_word_count</th>\n",
       "      <th>rf_uncertainty_score</th>\n",
       "      <th>rf_constraining_score</th>\n",
       "      <th>rf_positive_word_proportion</th>\n",
       "      <th>rf_negative_word_proportion</th>\n",
       "      <th>rf_uncertainty_word_proportion</th>\n",
       "      <th>rf_constraining_word_proportion</th>\n",
       "      <th>constraining_words_whole_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662.0</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803.0</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662.0</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805.0</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662.0</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808.0</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662.0</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811.0</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662.0</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811.0</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CIK            CONAME     FYRMO      FDATE     FORM  \\\n",
       "0  3662.0  SUNBEAM CORP/FL/  199803.0 1998-03-06  10-K405   \n",
       "1  3662.0  SUNBEAM CORP/FL/  199805.0 1998-05-15     10-Q   \n",
       "2  3662.0  SUNBEAM CORP/FL/  199808.0 1998-08-13  NT 10-Q   \n",
       "3  3662.0  SUNBEAM CORP/FL/  199811.0 1998-11-12   10-K/A   \n",
       "4  3662.0  SUNBEAM CORP/FL/  199811.0 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                   SECFNAME  mda_positive_score  \\\n",
       "0  edgar/data/3662/0000950170-98-000413.txt                   0   \n",
       "1  edgar/data/3662/0000950170-98-001001.txt                   0   \n",
       "2  edgar/data/3662/0000950172-98-000783.txt                   0   \n",
       "3  edgar/data/3662/0000950170-98-002145.txt                   0   \n",
       "4  edgar/data/3662/0000950172-98-001203.txt                   0   \n",
       "\n",
       "   mda_negative_score  mda_polarity_score  mda_average_sentence_length  ...  \\\n",
       "0                   0                   0                            0  ...   \n",
       "1                   0                   0                            0  ...   \n",
       "2                   0                   0                            0  ...   \n",
       "3                   0                   0                            0  ...   \n",
       "4                   0                   0                            0  ...   \n",
       "\n",
       "   rf_fog_index  rf_complex_word_count  rf_word_count  rf_uncertainty_score  \\\n",
       "0             0                      0              0                     0   \n",
       "1             0                      0              0                     0   \n",
       "2             0                      0              0                     0   \n",
       "3             0                      0              0                     0   \n",
       "4             0                      0              0                     0   \n",
       "\n",
       "   rf_constraining_score  rf_positive_word_proportion  \\\n",
       "0                      0                            0   \n",
       "1                      0                            0   \n",
       "2                      0                            0   \n",
       "3                      0                            0   \n",
       "4                      0                            0   \n",
       "\n",
       "   rf_negative_word_proportion  rf_uncertainty_word_proportion  \\\n",
       "0                            0                               0   \n",
       "1                            0                               0   \n",
       "2                            0                               0   \n",
       "3                            0                               0   \n",
       "4                            0                               0   \n",
       "\n",
       "   rf_constraining_word_proportion  constraining_words_whole_report  \n",
       "0                                0                                0  \n",
       "1                                0                                0  \n",
       "2                                0                                0  \n",
       "3                                0                                0  \n",
       "4                                0                                0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dictionary = pd.concat([df_orig,pd.DataFrame(columns=header_columns_with_sections)])\n",
    "data_dictionary = data_dictionary.fillna(0) #replacing all NaN values with zeroes\n",
    "data_dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "\n",
    "#NOTE: there is python error when parsing 64th and 65th url, so corresponding loops are skipped. see \"error_63&64index.png\"\n",
    "    \n",
    "#looping though each row\n",
    "for row_index in range(0,len(df.index)):\n",
    "    \n",
    "    #skipping row with index 63 and 64\n",
    "    if row_index==63 or row_index==64:\n",
    "        continue #error occurs when parsing 64th and 65th row, see \"error_63&64index.png\"\n",
    "    \n",
    "    #extract data\n",
    "    url = df['SECFNAME'][row_index]\n",
    "    data = extract_data_bs4(url)\n",
    "    \n",
    "    #loop through 3 sections (mda,qqdmr,rf)\n",
    "    section_index = 0\n",
    "    for section in section_full_names:\n",
    "        section_index = section_full_names.index(section)\n",
    "\n",
    "        #check if particular section (out of given 3) is present on webpage or not...\n",
    "        #...by matching \"table of contents\" value of webpage\n",
    "        #for example, \"ITEM 6.    MANAGEMENT'S DISCUSSION AND ANALYSIS\" will be checked with regex\n",
    "        pattern = 'ITEM [0-9a-zA-Z]+\\.([ ]{1,}|\\t)'+section\n",
    "\n",
    "        #check if regex pattern matches data from web page\n",
    "        if re.search(pattern,data,flags=re.IGNORECASE):\n",
    "            cleaned_data = clean_data(data) #removed all brackets and digits\n",
    "\n",
    "            cleaned_sent_orig = sent_tokenize(cleaned_data) #generate sentences\n",
    "\n",
    "            cleaned_data_sentences = []\n",
    "            for sentence in cleaned_sent_orig:\n",
    "                cleaned_data_sentences.append(re.sub('[\\.\\?]','',sentence)) #removing '.' and '?' from sentences\n",
    "\n",
    "            cleaned_data = re.sub('[\\.\\?]','',cleaned_data) #removing '.' and '?' for words\n",
    "            word_tokens = word_tokenize(cleaned_data) #tokenize words\n",
    "\n",
    "            stop_words = set(stopwords.words('english')) #nltk stopwords in english\n",
    "\n",
    "            cleaned_data_words = [word for word in word_tokens if word] #remove empty string values\n",
    "            cleaned_data_words = [word.lower() for word in cleaned_data_words] #converting all words into lowercase\n",
    "            cleaned_data_sentences = [sentence.lower() for sentence in cleaned_data_sentences] #converting all sentences into lowercase\n",
    "\n",
    "            #removing stopwords\n",
    "            cleaned_data_words = remove_stopwords(stop_words,cleaned_data_words)\n",
    "\n",
    "            #TOTAL SENTENCE COUNT\n",
    "            cleaned_data_sentences_length = len(cleaned_data_sentences) #total number of sentences\n",
    "            #TOTAL WORD COUNT\n",
    "            cleaned_data_words_length = len(cleaned_data_words) #total number of words\n",
    "\n",
    "\n",
    "            #******NEXT PART IS FOR CALCULATION OF VARIOUS FUNCTIONS******\n",
    "\n",
    "            #POSITIVE SCORE\n",
    "            positive_score = calculate_positive_score(cleaned_data_words)\n",
    "\n",
    "            #NEGATIVE SCORE\n",
    "            negative_score = calculate_negative_score(cleaned_data_words)\n",
    "\n",
    "            #POLARITY SCORE\n",
    "            polarity_score = calculate_polarity_score(positive_score, negative_score)\n",
    "\n",
    "            #AVERAGE SENTENCE LENGTH\n",
    "            sentence_avg_length = sentence_avg_len(cleaned_data_words_length, cleaned_data_sentences_length)\n",
    "\n",
    "            #COMPLEX WORD COUNT\n",
    "            complex_count = 0\n",
    "            for word in cleaned_data_words:\n",
    "                if check_complex(word):\n",
    "                    complex_count = complex_count + 1\n",
    "\n",
    "            #PERCENTAGE OF COMPLEX WORDS\n",
    "            percentage_complex_words = percent_complex_words(complex_count, cleaned_data_words_length)\n",
    "\n",
    "            #FOG INDEX\n",
    "            fog_index_value = fog_index(sentence_avg_length, percentage_complex_words)\n",
    "\n",
    "            #UNCERTAINITY SCORE\n",
    "            uncertainty_score = calculate_uncertainty_score(cleaned_data_words)\n",
    "\n",
    "            #CONSTRAINING SCORE\n",
    "            constraining_score = calculate_constraining_score(cleaned_data_words)\n",
    "\n",
    "            #POSITIVE WORD PROPORTION\n",
    "            positive_word_proportion = positive_score/cleaned_data_words_length\n",
    "\n",
    "            #NEGATIVE WORD PROPORTION\n",
    "            negative_word_proportion = negative_score/cleaned_data_words_length\n",
    "\n",
    "            #UNCERTAINTY WORD PROPORTION\n",
    "            uncertainty_word_proportion = uncertainty_score/cleaned_data_words_length\n",
    "\n",
    "            #CONSTRAINING WORD PROPORTION\n",
    "            constraining_word_proportion = constraining_score/cleaned_data_words_length\n",
    "\n",
    "            #Adding values back to final dataframe\n",
    "            data_dictionary.loc[data_dictionary.index[row_index],[section_names[section_index]+'_positive_score']] = positive_score\n",
    "            data_dictionary.loc[data_dictionary.index[row_index],[section_names[section_index]+'_negative_score']] = negative_score\n",
    "            data_dictionary.loc[data_dictionary.index[row_index],[section_names[section_index]+'_polarity_score']] = polarity_score\n",
    "            data_dictionary.loc[data_dictionary.index[row_index],[section_names[section_index]+'_average_sentence_length']] = sentence_avg_length\n",
    "            data_dictionary.loc[data_dictionary.index[row_index],[section_names[section_index]+'_percentage_of_complex_words']] = percentage_complex_words\n",
    "            data_dictionary.loc[data_dictionary.index[row_index],[section_names[section_index]+'_fog_index']] = fog_index_value\n",
    "            data_dictionary.loc[data_dictionary.index[row_index],[section_names[section_index]+'_complex_word_count']] = complex_count\n",
    "            data_dictionary.loc[data_dictionary.index[row_index],[section_names[section_index]+'_word_count']] = cleaned_data_words_length\n",
    "            data_dictionary.loc[data_dictionary.index[row_index],[section_names[section_index]+'_uncertainty_score']] = uncertainty_score\n",
    "            data_dictionary.loc[data_dictionary.index[row_index],[section_names[section_index]+'_constraining_score']] = constraining_score\n",
    "            data_dictionary.loc[data_dictionary.index[row_index],[section_names[section_index]+'_positive_word_proportion']] = positive_word_proportion\n",
    "            data_dictionary.loc[data_dictionary.index[row_index],[section_names[section_index]+'_negative_word_proportion']] = negative_word_proportion\n",
    "            data_dictionary.loc[data_dictionary.index[row_index],[section_names[section_index]+'_uncertainty_word_proportion']] = uncertainty_word_proportion\n",
    "            data_dictionary.loc[data_dictionary.index[row_index],[section_names[section_index]+'_constraining_word_proportion']] = constraining_word_proportion\n",
    "    #CONSTRAINING WORDS FOR WHOLE REPORT\n",
    "    constraining_words_report = constraining_words_whole_report(data)\n",
    "    data_dictionary.loc[data_dictionary.index[row_index],'constraining_words_whole_report'] = constraining_words_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "      <th>mda_positive_score</th>\n",
       "      <th>mda_negative_score</th>\n",
       "      <th>mda_polarity_score</th>\n",
       "      <th>mda_average_sentence_length</th>\n",
       "      <th>...</th>\n",
       "      <th>rf_fog_index</th>\n",
       "      <th>rf_complex_word_count</th>\n",
       "      <th>rf_word_count</th>\n",
       "      <th>rf_uncertainty_score</th>\n",
       "      <th>rf_constraining_score</th>\n",
       "      <th>rf_positive_word_proportion</th>\n",
       "      <th>rf_negative_word_proportion</th>\n",
       "      <th>rf_uncertainty_word_proportion</th>\n",
       "      <th>rf_constraining_word_proportion</th>\n",
       "      <th>constraining_words_whole_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662.0</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803.0</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "      <td>878</td>\n",
       "      <td>2463</td>\n",
       "      <td>-0.474409</td>\n",
       "      <td>18.116701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662.0</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805.0</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "      <td>445</td>\n",
       "      <td>1212</td>\n",
       "      <td>-0.462885</td>\n",
       "      <td>19.159197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662.0</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808.0</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662.0</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811.0</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "      <td>309</td>\n",
       "      <td>1174</td>\n",
       "      <td>-0.583277</td>\n",
       "      <td>15.017590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662.0</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811.0</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CIK            CONAME     FYRMO      FDATE     FORM  \\\n",
       "0  3662.0  SUNBEAM CORP/FL/  199803.0 1998-03-06  10-K405   \n",
       "1  3662.0  SUNBEAM CORP/FL/  199805.0 1998-05-15     10-Q   \n",
       "2  3662.0  SUNBEAM CORP/FL/  199808.0 1998-08-13  NT 10-Q   \n",
       "3  3662.0  SUNBEAM CORP/FL/  199811.0 1998-11-12   10-K/A   \n",
       "4  3662.0  SUNBEAM CORP/FL/  199811.0 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                   SECFNAME  mda_positive_score  \\\n",
       "0  edgar/data/3662/0000950170-98-000413.txt                 878   \n",
       "1  edgar/data/3662/0000950170-98-001001.txt                 445   \n",
       "2  edgar/data/3662/0000950172-98-000783.txt                   0   \n",
       "3  edgar/data/3662/0000950170-98-002145.txt                 309   \n",
       "4  edgar/data/3662/0000950172-98-001203.txt                   0   \n",
       "\n",
       "   mda_negative_score  mda_polarity_score  mda_average_sentence_length  ...  \\\n",
       "0                2463           -0.474409                    18.116701  ...   \n",
       "1                1212           -0.462885                    19.159197  ...   \n",
       "2                   0            0.000000                     0.000000  ...   \n",
       "3                1174           -0.583277                    15.017590  ...   \n",
       "4                   0            0.000000                     0.000000  ...   \n",
       "\n",
       "   rf_fog_index  rf_complex_word_count  rf_word_count  rf_uncertainty_score  \\\n",
       "0           0.0                      0              0                     0   \n",
       "1           0.0                      0              0                     0   \n",
       "2           0.0                      0              0                     0   \n",
       "3           0.0                      0              0                     0   \n",
       "4           0.0                      0              0                     0   \n",
       "\n",
       "   rf_constraining_score  rf_positive_word_proportion  \\\n",
       "0                      0                          0.0   \n",
       "1                      0                          0.0   \n",
       "2                      0                          0.0   \n",
       "3                      0                          0.0   \n",
       "4                      0                          0.0   \n",
       "\n",
       "   rf_negative_word_proportion  rf_uncertainty_word_proportion  \\\n",
       "0                          0.0                             0.0   \n",
       "1                          0.0                             0.0   \n",
       "2                          0.0                             0.0   \n",
       "3                          0.0                             0.0   \n",
       "4                          0.0                             0.0   \n",
       "\n",
       "   rf_constraining_word_proportion  constraining_words_whole_report  \n",
       "0                              0.0                             1275  \n",
       "1                              0.0                              871  \n",
       "2                              0.0                                5  \n",
       "3                              0.0                              613  \n",
       "4                              0.0                                4  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the output to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary.to_excel('SOLUTION_OUTPUT.xlsx') #requires openpyxl package"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
